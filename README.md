# KIVA-ICCV: Visual Analogy Learning

A PyTorch implementation of a Siamese Network for solving visual analogies, designed for the ICCV challenge.

## ğŸš€ Quick Start

### Prerequisites
```bash
# Install dependencies using uv
uv sync
```

This will automatically install all dependencies from your `pyproject.toml` file.

### Useful commands and basic cli usage

You can start by copying the `.env.example` file to `.env` and filling in the API token and project name.
```bash
cp .env.example .env
```

Main commands:
```
uv run --env-file .env python kiva-iccv/train.py --do_train --do_test --epochs 30 --batch_size 64
```
For the full list of options, run `uv run python kiva-iccv/train.py --help`.

Other useful commands:
```bash
make test-train-unit # use the unit test dataset to train and test the model
make test-overfit-validation # train and evaluate on validation set to make sure the model can learn to overfit
make fmt # format the code
```

## ğŸ“ Data

### Load the data

1. Load the basic competition data running:

```bash
uv run python kiva-iccv/utils/download.py
```

This will create the `data/` directory with the following structure:

**Note:** The `test` directory is not available yet. Will be released on September 1.

```
data/
â”œâ”€â”€ train/
â”œâ”€â”€ validation/
â”œâ”€â”€ test/
â”œâ”€â”€ unit/
â”œâ”€â”€ train.json
â”œâ”€â”€ validation.json
â”œâ”€â”€ test.json
â””â”€â”€ unit.json
```

2. Run the following commands to create the subimages that will be used by the model.
```bash
for dataset in train validation test unit; do
    uv run python kiva-iccv/utils/transform.py --dataset $dataset
done
```

3. After this, the `data/` directory will have the following structure:
```
data/
â”œâ”€â”€ train/           # Training images
|   â”œâ”€â”€ sample1.jpg
|   â”œâ”€â”€ sample2.jpg
|   â””â”€â”€ ...
â”œâ”€â”€ validation/      # Validation images
|   â”œâ”€â”€ sample1.jpg
|   â”œâ”€â”€ sample2.jpg
|   â””â”€â”€ ...
â”œâ”€â”€ test/           # Test images
|   â”œâ”€â”€ sample1.jpg
|   â”œâ”€â”€ sample2.jpg
|   â””â”€â”€ ...
â”œâ”€â”€ split_train/           # Split training images
â”‚   â”œâ”€â”€ sample1_ex_before.jpg
â”‚   â”œâ”€â”€ sample1_ex_after.jpg
â”‚   â”œâ”€â”€ sample1_test_before.jpg
â”‚   â”œâ”€â”€ sample1_choice_a.jpg
â”‚   â”œâ”€â”€ sample1_choice_b.jpg
â”‚   â””â”€â”€ sample1_choice_c.jpg
â”œâ”€â”€ split_validation/      # Split validation images
|   â””â”€â”€ ...
â”œâ”€â”€ split_test/           # Split test images
|   â””â”€â”€ ...
â”œâ”€â”€ train.json            # Training metadata
â”œâ”€â”€ validation.json       # Validation metadata
â””â”€â”€ test.json            # Test metadata
```

### Image Naming Convention
Each sample requires 6 images:
- `{sample_id}_ex_before.jpg` - Example "before" image
- `{sample_id}_ex_after.jpg` - Example "after" image  
- `{sample_id}_test_before.jpg` - Test "before" image
- `{sample_id}_choice_a.jpg` - Choice A image
- `{sample_id}_choice_b.jpg` - Choice B image
- `{sample_id}_choice_c.jpg` - Choice C image
